{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP9DVj0WNJp5FQd4OmWPPPV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzvP83xGNCPn","executionInfo":{"status":"ok","timestamp":1723023568595,"user_tz":-330,"elapsed":6177,"user":{"displayName":"abineha prabu","userId":"10393966300941890966"}},"outputId":"310a554c-45c7-4ce9-c29b-dd095b33ba25"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'neural-fingerprint'...\n","remote: Enumerating objects: 1760, done.\u001b[K\n","remote: Total 1760 (delta 0), reused 0 (delta 0), pack-reused 1760\u001b[K\n","Receiving objects: 100% (1760/1760), 51.21 MiB | 12.63 MiB/s, done.\n","Resolving deltas: 100% (1141/1141), done.\n"]}],"source":["!git clone https://github.com/HIPS/neural-fingerprint.git\n"]},{"cell_type":"code","source":["%cd neural-fingerprint\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7F6v-G7HNKEV","executionInfo":{"status":"ok","timestamp":1723023586916,"user_tz":-330,"elapsed":627,"user":{"displayName":"abineha prabu","userId":"10393966300941890966"}},"outputId":"fcb8b22c-7e5d-40df-d9e6-ad575f01156b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/neural-fingerprint\n"]}]},{"cell_type":"code","source":["!pip2 install -r requirements.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6M2mg9gNKGj","executionInfo":{"status":"ok","timestamp":1723023597860,"user_tz":-330,"elapsed":626,"user":{"displayName":"abineha prabu","userId":"10393966300941890966"}},"outputId":"2d447573-f738-4a2e-8578-8faf28d6e09e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: pip2: command not found\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DIYqaxgqNXnl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BO7D397_NXpv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XDBiBMyhNXtk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AINzRfDNNKIu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example regression script using neural fingerprints.\n","#\n","# Compares Morgan fingerprints to neural fingerprints.\n","\n","import autograd.numpy as np\n","import autograd.numpy.random as npr\n","\n","from neuralfingerprint import load_data\n","from neuralfingerprint import build_morgan_deep_net\n","from neuralfingerprint import build_conv_deep_net\n","from neuralfingerprint import normalize_array, adam\n","from neuralfingerprint import build_batched_grad\n","from neuralfingerprint.util import rmse\n","\n","from autograd import grad\n","\n","task_params = {'target_name' : 'measured log solubility in mols per litre',\n","               'data_file'   : 'delaney.csv'}\n","N_train = 800\n","N_val   = 20\n","N_test  = 20\n","\n","model_params = dict(fp_length=50,    # Usually neural fps need far fewer dimensions than morgan.\n","                    fp_depth=4,      # The depth of the network equals the fingerprint radius.\n","                    conv_width=20,   # Only the neural fps need this parameter.\n","                    h1_size=100,     # Size of hidden layer of network on top of fps.\n","                    L2_reg=np.exp(-2))\n","train_params = dict(num_iters=100,\n","                    batch_size=100,\n","                    init_scale=np.exp(-4),\n","                    step_size=np.exp(-6))\n","\n","# Define the architecture of the network that sits on top of the fingerprints.\n","vanilla_net_params = dict(\n","    layer_sizes = [model_params['fp_length'], model_params['h1_size']],  # One hidden layer.\n","    normalize=True, L2_reg = model_params['L2_reg'], nll_func = rmse)\n","\n","def train_nn(pred_fun, loss_fun, num_weights, train_smiles, train_raw_targets, train_params, seed=0,\n","             validation_smiles=None, validation_raw_targets=None):\n","    \"\"\"loss_fun has inputs (weights, smiles, targets)\"\"\"\n","    print \"Total number of weights in the network:\", num_weights\n","    init_weights = npr.RandomState(seed).randn(num_weights) * train_params['init_scale']\n","\n","    num_print_examples = 100\n","    train_targets, undo_norm = normalize_array(train_raw_targets)\n","    training_curve = []\n","    def callback(weights, iter):\n","        if iter % 10 == 0:\n","            print \"max of weights\", np.max(np.abs(weights))\n","            train_preds = undo_norm(pred_fun(weights, train_smiles[:num_print_examples]))\n","            cur_loss = loss_fun(weights, train_smiles[:num_print_examples], train_targets[:num_print_examples])\n","            training_curve.append(cur_loss)\n","            print \"Iteration\", iter, \"loss\", cur_loss,\\\n","                  \"train RMSE\", rmse(train_preds, train_raw_targets[:num_print_examples]),\n","            if validation_smiles is not None:\n","                validation_preds = undo_norm(pred_fun(weights, validation_smiles))\n","                print \"Validation RMSE\", iter, \":\", rmse(validation_preds, validation_raw_targets),\n","\n","    # Build gradient using autograd.\n","    grad_fun = grad(loss_fun)\n","    grad_fun_with_data = build_batched_grad(grad_fun, train_params['batch_size'],\n","                                            train_smiles, train_targets)\n","\n","    # Optimize weights.\n","    trained_weights = adam(grad_fun_with_data, init_weights, callback=callback,\n","                           num_iters=train_params['num_iters'], step_size=train_params['step_size'])\n","\n","    def predict_func(new_smiles):\n","        \"\"\"Returns to the original units that the raw targets were in.\"\"\"\n","        return undo_norm(pred_fun(trained_weights, new_smiles))\n","    return predict_func, trained_weights, training_curve\n","\n","\n","def main():\n","    print \"Loading data...\"\n","    traindata, valdata, testdata = load_data(\n","        task_params['data_file'], (N_train, N_val, N_test),\n","        input_name='smiles', target_name=task_params['target_name'])\n","    train_inputs, train_targets = traindata\n","    val_inputs,   val_targets   = valdata\n","    test_inputs,  test_targets  = testdata\n","\n","    def print_performance(pred_func):\n","        train_preds = pred_func(train_inputs)\n","        val_preds = pred_func(val_inputs)\n","        print \"\\nPerformance (RMSE) on \" + task_params['target_name'] + \":\"\n","        print \"Train:\", rmse(train_preds, train_targets)\n","        print \"Test: \", rmse(val_preds,  val_targets)\n","        print \"-\" * 80\n","        return rmse(val_preds, val_targets)\n","\n","    def run_morgan_experiment():\n","        loss_fun, pred_fun, net_parser = \\\n","            build_morgan_deep_net(model_params['fp_length'],\n","                                  model_params['fp_depth'], vanilla_net_params)\n","        num_weights = len(net_parser)\n","        predict_func, trained_weights, conv_training_curve = \\\n","            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n","                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n","        return print_performance(predict_func)\n","\n","    def run_conv_experiment():\n","        conv_layer_sizes = [model_params['conv_width']] * model_params['fp_depth']\n","        conv_arch_params = {'num_hidden_features' : conv_layer_sizes,\n","                            'fp_length' : model_params['fp_length'], 'normalize' : 1}\n","        loss_fun, pred_fun, conv_parser = \\\n","            build_conv_deep_net(conv_arch_params, vanilla_net_params, model_params['L2_reg'])\n","        num_weights = len(conv_parser)\n","        predict_func, trained_weights, conv_training_curve = \\\n","            train_nn(pred_fun, loss_fun, num_weights, train_inputs, train_targets,\n","                     train_params, validation_smiles=val_inputs, validation_raw_targets=val_targets)\n","        test_predictions = predict_func(test_inputs)\n","        return rmse(test_predictions, test_targets)\n","\n","    print \"Task params\", task_params\n","    print\n","    print \"Starting Morgan fingerprint experiment...\"\n","    test_loss_morgan = run_morgan_experiment()\n","    print \"Starting neural fingerprint experiment...\"\n","    test_loss_neural = run_conv_experiment()\n","    print\n","    print \"Morgan test RMSE:\", test_loss_morgan, \"Neural test RMSE:\", test_loss_neural\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"iyjwoOeqNKMF","executionInfo":{"status":"error","timestamp":1723023545836,"user_tz":-330,"elapsed":5,"user":{"displayName":"abineha prabu","userId":"10393966300941890966"}},"outputId":"20d63a0f-aca3-4bc2-f79e-40659e810062"},"execution_count":1,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"Missing parentheses in call to 'print'. Did you mean print(...)? (<ipython-input-1-193db3455f95>, line 41)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-193db3455f95>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    print \"Total number of weights in the network:\", num_weights\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(...)?\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"01J2ttZJNKjC"},"execution_count":null,"outputs":[]}]}